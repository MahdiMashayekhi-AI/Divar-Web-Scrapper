import time
import csv
import random
import config
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    WebDriverException,
    TimeoutException,
    NoSuchElementException,
)

class ColorLogger:
    """A simple logger for colored console output."""
    COLORS = {
        "INFO": "\033[92m",
        "WARNING": "\033[93m",
        "ERROR": "\033[91m",
        "COLLECTOR": "\033[94m",
        "SCRAPER": "\033[96m",
        "RESET": "\033[0m",
    }

    @staticmethod
    def log(message, level="INFO"):
        """Prints a message to the console with specified color."""
        color = ColorLogger.COLORS.get(level.upper(), ColorLogger.COLORS["INFO"])
        print(f"{color}{message}{ColorLogger.COLORS['RESET']}")

class DivarScraper:
    """
    A robust, batch-processing web scraper for Divar.ir, incorporating user-refined logic.
    """

    def __init__(self):
        """Initializes the scraper."""
        self.csv_filename = config.CSV_FILE
        self.visited_links_file = config.VISITED_LINKS_FILE
        self.visited_links = set()
        self.driver = None
        self.scraped_data_buffer = []
        self.total_scraped_count = 0
        self.load_and_count_visited_links()
        
        # Headers now match the keys generated by the scraping function.
        self.csv_headers = [
            'title', 'price', 'description', 'time_posted', 'location', 'link',
            'scraped_time', 'meterage', 'build_year', 'rooms', 'floor',
            'price_per_meter', 'has_parking', 'has_warehouse', 
            'has_balcony', 'has_elevator'
        ]

    def _normalize_persian_numbers(self, text: str) -> str:
        """Converts Persian digits to English and cleans the string."""
        if not text:
            return None
        persian_to_english_map = str.maketrans('۰۱۲۳۴۵۶۷۸۹', '0123456789')
        normalized_text = text.translate(persian_to_english_map)

        if ' ' in normalized_text:
            normalized_text = normalized_text.split(' ')[0]
        cleaned_text = ''.join(filter(str.isdigit, normalized_text))
        return cleaned_text if cleaned_text else None

    def _random_sleep(self, multiplier=1):
        """Pauses for a random duration."""
        sleep_time = random.uniform(config.MIN_SLEEP_TIME, config.MAX_SLEEP_TIME) * multiplier
        ColorLogger.log(f"[INFO] Sleeping for {sleep_time:.2f} seconds...", "INFO")
        time.sleep(sleep_time)

    def load_and_count_visited_links(self):
        """Loads visited links and counts existing records."""
        try:
            if os.path.exists(self.visited_links_file):
                with open(self.visited_links_file, 'r', encoding='utf-8') as file:
                    self.visited_links = set(line.strip() for line in file if line.strip())
            
            if os.path.exists(self.csv_filename):
                with open(self.csv_filename, 'r', encoding='utf-8-sig') as csvfile:
                    reader = csv.reader(csvfile)
                    # Subtract 1 for the header row if it exists
                    self.total_scraped_count = max(0, sum(1 for row in reader) - 1)
            
            ColorLogger.log(f"[INFO] Found {self.total_scraped_count} previously scraped records.")
            ColorLogger.log(f"[INFO] Loaded {len(self.visited_links)} unique links from visited_links.txt.")
        except Exception as e:
            ColorLogger.log(f"[ERROR] Failed to load or count previous data: {e}.", "ERROR")

    def save_visited_link(self, link):
        """Saves a link to the visited links file."""
        try:
            with open(self.visited_links_file, 'a', encoding='utf-8') as file:
                file.write(link + "\n")
        except IOError as e:
            ColorLogger.log(f"[ERROR] Could not write to visited links file: {e}", "ERROR")

    def init_driver(self):
        """Initializes the Selenium WebDriver."""
        chrome_options = Options()
        # chrome_options.add_argument("--headless=new") # Uncomment if you want run without GUI
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        
        service = Service()
        try:
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(config.PAGE_LOAD_TIMEOUT)
            ColorLogger.log("[INFO] WebDriver initialized successfully.", "INFO")
        except WebDriverException as e:
            ColorLogger.log(f"[ERROR] WebDriver initialization failed: {e}", "ERROR")
            raise

    def quit_driver(self):
        """Quits the WebDriver."""
        if self.driver:
            self.driver.quit()
            self.driver = None
            ColorLogger.log("[INFO] WebDriver quit.", "INFO")

    def write_csv_batch(self):
        """Writes buffered data to the CSV."""
        if not self.scraped_data_buffer: return
        file_exists = os.path.exists(self.csv_filename) and os.path.getsize(self.csv_filename) > 0
        try:
            with open(self.csv_filename, 'a', newline='', encoding='utf-8-sig') as file:
                writer = csv.DictWriter(file, fieldnames=self.csv_headers)
                if not file_exists: writer.writeheader()
                writer.writerows(self.scraped_data_buffer)
            ColorLogger.log(f"[INFO] Saved {len(self.scraped_data_buffer)} records to {self.csv_filename}.", "INFO")
            self.scraped_data_buffer.clear()
        except IOError as e:
            ColorLogger.log(f"[ERROR] Failed to write to CSV file: {e}", "ERROR")

    def close_map_overlay(self):
        """Closes the map overlay."""
        try:
            close_map_button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//div[contains(@class, 'absolute-c06f1')]"))
            )
            self.driver.execute_script("arguments[0].click();", close_map_button)
            ColorLogger.log("[INFO] Map overlay closed successfully.", "INFO")
            self._random_sleep()
        except TimeoutException:
            ColorLogger.log("[INFO] Map overlay not found, continuing...", "INFO")

    def collect_link_batch(self, batch_size):
        """Collects a batch of new links from ads with a visible price."""
        batch_links = []
        no_new_links_streak = 0
        
        if config.SEARCH_URL not in self.driver.current_url:
             self.driver.get(config.SEARCH_URL)
             self.close_map_overlay()

        while len(batch_links) < batch_size:
            ColorLogger.log(f"[Collector] Links for current batch: {len(batch_links)}/{batch_size}", "COLLECTOR")
            
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            for _ in range(config.SCROLL_ATTEMPTS_PER_CYCLE):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self._random_sleep(0.5)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height: break
                last_height = new_height

            try:
                load_more_button = WebDriverWait(self.driver, 5).until(
                    EC.element_to_be_clickable((By.XPATH, "//button/span[text()='آگهی‌های بیشتر']"))
                )
                self.driver.execute_script("arguments[0].click();", load_more_button)
                ColorLogger.log("[INFO] 'Load More Ads' button clicked.", "INFO")
                self._random_sleep(2)
            except Exception:
                ColorLogger.log("[INFO] 'Load More Ads' button not found this cycle.", "INFO")
            
            newly_found_this_cycle = 0
            try:
                ad_cards = self.driver.find_elements(By.XPATH, "//article[contains(@class, 'unsafe-kt-post-card')]")
                for card in ad_cards:
                    try:
                        link_element = card.find_element(By.TAG_NAME, 'a')
                        href = link_element.get_attribute('href')
                        full_link = href if href.startswith('http') else config.BASE_URL + href
                        
                        if full_link in self.visited_links:
                            continue

                        price_element = card.find_element(By.CLASS_NAME, 'unsafe-kt-post-card__description')
                        price_text = price_element.text
                        if 'توافقی' not in price_text and any(char.isdigit() for char in price_text):
                            self.visited_links.add(full_link)
                            batch_links.append(full_link)
                            newly_found_this_cycle += 1
                            if len(batch_links) >= batch_size: break
                    except NoSuchElementException:
                        continue
            except Exception as e:
                ColorLogger.log(f"[ERROR] Error during link collection: {e}", "ERROR")

            if newly_found_this_cycle == 0:
                no_new_links_streak += 1
                if no_new_links_streak >= config.NO_NEW_ADS_BREAK_THRESHOLD:
                    ColorLogger.log("[Collector] Ending collection: No new ads with prices found.", "WARNING")
                    return batch_links
            else:
                no_new_links_streak = 0
            
            if len(batch_links) >= batch_size: break

        return batch_links

    def scrape_ad_details(self, link):
        """Scrapes details from a single ad page using robust, user-refined logic."""
        ColorLogger.log(f"[Scraper] Processing: {link}", "SCRAPER")
        ad_data = {field: None for field in self.csv_headers}
        ad_data['link'] = link
        ad_data['scraped_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            self.driver.get(link)
            self._random_sleep(0.5)
            WebDriverWait(self.driver, config.WAIT_TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, 'h1')))

            try:
                ad_data['title'] = self.driver.find_element(By.TAG_NAME, 'h1').text.strip()
            except NoSuchElementException:
                ColorLogger.log(f"[WARNING] Title not found for {link}", "WARNING")

            try:
                subtitle = self.driver.find_element(By.CLASS_NAME, 'kt-page-title__subtitle').text.strip()
                parts = subtitle.split(' در ', 1)
                ad_data['time_posted'] = parts[0].strip() if len(parts) > 1 else 'N/A'
                ad_data['location'] = parts[1].strip() if len(parts) > 1 else subtitle
            except NoSuchElementException:
                ColorLogger.log(f"[WARNING] Subtitle not found for {link}", "WARNING")

            try:
                desc = self.driver.find_element(By.CLASS_NAME, 'kt-description-row__text').text.strip()
                ad_data['description'] = desc.replace('\n', ' ')
            except NoSuchElementException:
                ColorLogger.log(f"[WARNING] Description not found for {link}", "WARNING")

            try:
                rows = self.driver.find_elements(By.XPATH, '(//table[contains(@class, "kt-group-row")])[1]//td')
                if len(rows) >= 3:
                    ad_data['meterage'] = self._normalize_persian_numbers(rows[0].text)
                    ad_data['build_year'] = self._normalize_persian_numbers(rows[1].text)
                    ad_data['rooms'] = self._normalize_persian_numbers(rows[2].text)
            except Exception:
                 ColorLogger.log(f"[WARNING] Main spec table not found for {link}", "WARNING")

            try:
                rows = self.driver.find_elements(By.CLASS_NAME, 'kt-unexpandable-row')
                for row in rows:
                    try:
                        title = row.find_element(By.CLASS_NAME, 'kt-unexpandable-row__title').text.strip()
                        value = row.find_element(By.CLASS_NAME, 'kt-unexpandable-row__value').text.strip()
                        if "قیمت کل" in title:
                            ad_data['price'] = self._normalize_persian_numbers(value)
                        elif "قیمت هر متر" in title:
                            ad_data['price_per_meter'] = self._normalize_persian_numbers(value)
                        elif "طبقه" in title:
                            ad_data['floor'] = self._normalize_persian_numbers(value)
                    except NoSuchElementException:
                        continue
            except Exception:
                ColorLogger.log(f"[WARNING] Key-value list not found for {link}", "WARNING")
            
            # Reset features to avoid carrying over from previous ads
            ad_data['has_parking'] = False
            ad_data['has_warehouse'] = False
            ad_data['has_balcony'] = False
            ad_data['has_elevator'] = False
            try:
                features_section = self.driver.find_element(By.XPATH, '//span[text()="ویژگی‌ها و امکانات"]/ancestor::div[contains(@class,"post-page__section")]')
                features_text = features_section.text
                if 'پارکینگ' in features_text: ad_data['has_parking'] = True
                if 'انباری' in features_text: ad_data['has_warehouse'] = True
                if 'بالکن' in features_text: ad_data['has_balcony'] = True
                if 'آسانسور' in features_text: ad_data['has_elevator'] = True
            except NoSuchElementException:
                ColorLogger.log(f"[WARNING] Amenities section not found for {link}", "WARNING")

            if not ad_data['title'] or not ad_data['meterage'] or not ad_data['price']:
                ColorLogger.log(f"[QUALITY FAILED] Skipping ad due to missing critical data. "
                                f"Title: {ad_data['title']}, Meterage: {ad_data['meterage']}, Price: {ad_data['price']}. Link: {link}", "ERROR")
                return None
            
            ColorLogger.log(f"[SUCCESS] Scraped: {ad_data.get('title', 'N/A')}", "INFO")
            return ad_data

        except Exception as e:
            ColorLogger.log(f"[ERROR] Critical failure scraping details for {link}. Error: {e}", "ERROR")
            return None
    
    def run(self):
        """Main orchestrator for the batch-processing scraping."""
        self.init_driver()
        
        try:
            while self.total_scraped_count < config.TARGET_AD_COUNT:
                ColorLogger.log(f"\n--- CYCLE START --- Total Scraped: {self.total_scraped_count}/{config.TARGET_AD_COUNT} ---", "INFO")
                
                ColorLogger.log("[INFO] Starting to collect a new batch of links...", "INFO")
                new_links_batch = self.collect_link_batch(config.LINK_COLLECTION_BATCH_SIZE)
                
                if not new_links_batch:
                    ColorLogger.log("[INFO] No new links could be collected. Ending scraper.", "INFO")
                    break

                ColorLogger.log(f"\n[INFO] Collected a batch of {len(new_links_batch)} links. Now scraping details.", "INFO")
                
                for link in new_links_batch:
                    if self.total_scraped_count >= config.TARGET_AD_COUNT:
                        ColorLogger.log("[INFO] Target ad count reached. Stopping.", "INFO")
                        break
                    
                    details = self.scrape_ad_details(link)
                    if details:
                        self.scraped_data_buffer.append(details)
                        self.save_visited_link(link)
                        self.total_scraped_count += 1

                        if len(self.scraped_data_buffer) >= config.BATCH_SAVE_SIZE:
                            self.write_csv_batch()

                self.write_csv_batch()
            
            ColorLogger.log("\n--- SCRAPING PROCESS FINISHED ---", "INFO")
        except Exception as e:
            ColorLogger.log(f"[CRITICAL] A top-level exception occurred: {e}", "ERROR")
        finally:
            self.write_csv_batch()
            self.quit_driver()

if __name__ == "__main__":
    scraper = DivarScraper()
    scraper.run()
